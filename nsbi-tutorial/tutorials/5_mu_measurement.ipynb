{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d7d732",
   "metadata": {},
   "source": [
    "# Measurement of the signal strength\n",
    "\n",
    "$$\\mathcal{L} (\\mu | x) = \\prod_{i}^{n} p (x_i | \\mu) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "733fc2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from physics.simulation import mcfm\n",
    "from physics.analysis import zz4l, zz2l2v\n",
    "from physics.hstar import sigstr\n",
    "from nsbi import carl\n",
    "\n",
    "import matplotlib, matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d959247",
   "metadata": {},
   "source": [
    "# 0. Define the parameter space\n",
    "\n",
    "The first thing to do is to define the parameter space that will define the set of hypotheses we wish to test.\n",
    "For us, this is the signal strength parameter, which we can safely take to be between 0 and 4:\n",
    "\n",
    "$$ 0 \\leq \\mu \\leq 4 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3196c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_space = torch.linspace(0.0, 4.0, 401)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de297c75",
   "metadata": {},
   "source": [
    "# 1. Open the \"mystery\" dataset\n",
    "\n",
    "A dataset containing events generated according to an unknown (to you) value of $\\mu$, has been prepared. We are going to open this dataset and read out for each event its (1) observables, and (2) number of occurrences.\n",
    "\n",
    "Note: Do you notice anything strange about (2)? In a real LHC dataset, do you think it's likely for there to be two events with *exactly* the same features? What about non-integer-valued occurences of an event? While these are fundamental differences between a simulated and real dataset, they will not matter for our purposes of performing inference on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc76834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(...)\n",
    "features = [...]\n",
    "\n",
    "kinematics = torch.tensor(df[features])\n",
    "nevents    = torch.tensor(df['nevents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e5855",
   "metadata": {},
   "source": [
    "## 2. Evaluating the likelihood: rate term\n",
    "\n",
    "As already introduced, our likelihood function is composed of two terms, the first of which can be readily evaluated as\n",
    "\n",
    "$$\\mathcal{L}_{\\mathrm{rate}}(\\mu | \\mathcal{D}) = \\mathcal{P}(n ; \\nu(\\mu)) = \\frac{\\nu^{n}(\\mu) e^{-\\nu(\\mu)}}{n!},$$\n",
    "\n",
    "where $n$ is the total number of events in the observed dataset, and $\\nu(\\mu)$ is the expected number of events, which of course depends on the $\\mu$-hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5584c3",
   "metadata": {},
   "source": [
    "### 2.(a) Compute the total observed number of events, $n$\n",
    "\n",
    "Task: Add up all number of events in the dataset to obtaine the total number of events observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760247e",
   "metadata": {},
   "source": [
    "nu_obs = torch.sum(nevents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1f613",
   "metadata": {},
   "source": [
    "### 2.(b) Compute the total expected number of events, $\\nu(\\mu = 1)$\n",
    "\n",
    "The expected number of events is given by the cross section time luminosity:\n",
    "\n",
    "$$\\nu = \\sigma \\times L$$\n",
    "\n",
    "where the cross section, of course, depends on the parameter. \n",
    "The predicted \"visual\" cross sections, i.e. after detector acceptance/efficiency effects, of the total $gg\\to(h^{\\ast}\\to)ZZ\\to 4\\ell$ process, as well as its individual contributions from the signal, background and interference terms, are already calculated and available.\n",
    "Let's compute the expected number of events under the SM hypothesis, assuming HL-LHC luminosity of $3000\\,\\mathrm{fb}^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295465d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi = 3000.0  # ifb\n",
    "\n",
    "with open('data/xsec.json', 'r') as f:\n",
    "    xs = json.load(f)  # fb\n",
    "    xs_sig_sm = np.prod(xs['sig'])\n",
    "    xs_bkg_sm = np.prod(xs['bkg'])\n",
    "    xs_int_sm = np.prod(xs['int'])\n",
    "\n",
    "nu_sig_sm = xs_sig_sm * lumi\n",
    "nu_bkg_sm = xs_bkg_sm * lumi\n",
    "nu_int_sm = xs_int_sm * lumi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a809974",
   "metadata": {},
   "source": [
    "$$\\nu_{\\mathrm{sbi}}(\\mu) = \\mu \\nu_{\\mathrm{s}}(1) + \\sqrt{\\mu} \\nu_{\\mathrm{i}}(1) + \\nu_{\\mathrm{b}}(1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda5f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_sig_mu = nu_sig_sm * mu_space\n",
    "nu_int_mu = nu_int_sm * mu_space\n",
    "nu_sbi_mu = nu_sig_mu + nu_int_mu + nu_bkg_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142778d",
   "metadata": {},
   "source": [
    "### 2.(d) Define & compute the Poisson likelihood\n",
    "\n",
    "Using the quantities computed above, we can now compute the negative log likelihood (NLL) of the rate term as a function of $\\mu$:\n",
    "\n",
    "$$- \\log \\mathcal{L}_{\\mathrm{rate}}(\\mu | \\mathcal{D})  = \\nu(\\mu) - n \\log\\nu(\\mu) + \\log(n!)$$\n",
    "\n",
    "Reminder: the \"disappearance\" of $-\\log (1/n!)$ term does not affect the minimization of NLL as a function of $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_negative_log_likelihood(n_obs, nu_exp):\n",
    "    \"\"\"\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ffb61852",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mu_space, t_rate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505c900",
   "metadata": {},
   "source": [
    "## 3. Evaluating the likelihood (ratio): shape term\n",
    "\n",
    "Here comes NSBI, which will estimate the shape term of our likelihood, and (hopefully) improve our results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada18999",
   "metadata": {},
   "source": [
    "### 3.(a) Load the NN models\n",
    "\n",
    "Let's first load the CARL models that we've trained in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bf897c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = 'run/h4l'\n",
    "_, _, scaler_sbi_over_bkg, model_sbi_over_bkg = carl.utils.load_results(run_dir, 'sbi_over_bkg')\n",
    "_, _, scaler_sig_over_bkg, model_sig_over_bkg = carl.utils.load_results(run_dir, 'sig_over_bkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2626562",
   "metadata": {},
   "source": [
    "### 3.(b) Run the models over the dataset, and perform the likelihood ratio trick\n",
    "\n",
    "This part should also be straightforward, given the previous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc88df1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'carl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m r_sig_over_bkg_sm \u001b[38;5;241m=\u001b[39m \u001b[43mcarl\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_likelihood_ratio(events, features, scaler_sig_over_bkg, model_sig_over_bkg)\n\u001b[1;32m      2\u001b[0m r_sbi_over_bkg_sm \u001b[38;5;241m=\u001b[39m carl\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_likelihood_ratio(events, features, scaler_sbi_over_bkg, model_sbi_over_bkg)\n\u001b[1;32m      4\u001b[0m s_sig_over_bkg_sm \u001b[38;5;241m=\u001b[39m model_sig_over_bkg\u001b[38;5;241m.\u001b[39mpredict(scaler_sig_over_bkg\u001b[38;5;241m.\u001b[39mtransform(events\u001b[38;5;241m.\u001b[39mkinematics[features]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'carl' is not defined"
     ]
    }
   ],
   "source": [
    "r_sig_over_bkg_sm = carl.utils.get_likelihood_ratio(events, features, scaler_sig_over_bkg, model_sig_over_bkg)\n",
    "r_sbi_over_bkg_sm = carl.utils.get_likelihood_ratio(events, features, scaler_sbi_over_bkg, model_sbi_over_bkg)\n",
    "\n",
    "s_sig_over_bkg_sm = model_sig_over_bkg.predict(scaler_sig_over_bkg.transform(events.kinematics[features]))\n",
    "r_sig_over_bkg_sm = s_sig_over_bkg_sm / (1 - s_sig_over_bkg_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce176da",
   "metadata": {},
   "source": [
    "### 3.(c) Evaluate the probability density ratio\n",
    "\n",
    "$$\n",
    "\\frac{p_{\\rm sbi} (x | \\mu)}{p_{\\rm bkg} (x)} = \\frac{ (\\mu - \\sqrt{\\mu}) \\sigma_{\\rm sig} r_{\\rm sig} + \\sqrt{\\mu} \\sigma_{\\rm sbi} r_{\\rm sbi} + (1-\\sqrt{\\mu}) \\sigma_{\\rm bkg} }{ \\mu \\sigma_{\\rm sig} + \\sqrt{\\mu} \\sigma_{\\rm int} + \\sigma_{\\rm bkg} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc1aea",
   "metadata": {},
   "source": [
    "Tip: if you want to compute all elements of the $N \\times M$ tensor where $N$ is the number of entries in the dataset and $M$ is the number of $\\mu$ values being tested, then tensor broadcasting is\n",
    "```py\n",
    "a.shape  # (N,)\n",
    "b.shape  # (M,)\n",
    "c = a[:,None] * b[None,:]\n",
    "c.shape  # (N, M)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier_sig = mu_space - torch.sqrt(mu_space)\n",
    "multiplier_sbi = torch.sqrt(mu_space)\n",
    "multiplier_bkg = 1 - torch.sqrt(mu_space)\n",
    "\n",
    "r_sbi_over_bkg_mu = ( xs_sig * multiplier_sig[None,:] * r_sig_over_bkg_sm[:,None] + xs_sbi * multiplier_sbi[None,:] * r_sbi_over_bkg_sm[:,None] + xs_bkg * multiplier_bkg[None,:] ) / (xs_sig * mu_space + xs_int * torch.sqrt(mu_space) + xs_bkg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5678fad",
   "metadata": {},
   "source": [
    "Evaluate $t_{shape} = - \\sum \\log_{i}^{n} (\\frac{p_{\\rm sbi} (x | \\mu)}{p_{\\rm bkg} (x)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7027de",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_shape = -2 * torch.sum(lhc_lumi * torch.tensor(w_asimov)[:,None] * torch.log(r_sbi_over_bkg_mu), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c62e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mu_space, t_shape)\n",
    "plt.show()\n",
    "print(torch.min(t_shape))\n",
    "print(mu_space[torch.argmin(t_shape)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02dfc94",
   "metadata": {},
   "source": [
    "# Evaluating the likelihood (ratio): rate + shape\n",
    "\n",
    "$$ t = t_{\\rm rate} + t_{\\rm shape} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "72a314d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t_rate + t_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d5e88d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mu_space, t)\n",
    "plt.show()\n",
    "print(torch.min(t_shape))\n",
    "print(mu_space[torch.argmin(t_shape)])\n",
    "print(t[0] - torch.min(t))\n",
    "print(mu_space[torch.argmin(t)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
