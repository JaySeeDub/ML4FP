{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e04280",
   "metadata": {},
   "source": [
    "# NSBI training\n",
    "\n",
    "In this tutorial, you will be training surrogate neural networks that estimate the ratio of probability densities of an event under different hypotheses. These are special kind of classifiers known as CARL models.\n",
    "\n",
    "$$\n",
    "r( x | \\theta_1, \\theta_2 ) \\equiv \\frac{p(x | \\theta_1)}{p( x | \\theta_2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf547bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import lightning as L\n",
    "\n",
    "from physics.simulation import mcfm\n",
    "from physics.analysis import zz4l, zz2l2v\n",
    "from physics.hstar import sigstr\n",
    "from nsbi import carl\n",
    "\n",
    "import matplotlib, matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1eb0fe",
   "metadata": {},
   "source": [
    "## 1. Preparing the training datasets\n",
    "\n",
    "The training data consists of examples drawn from the two hypotheses of which we wish to estimate the ratio of. They will correspondingly be referred to as the numerator and denominator hypotheses from this point on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'run/h4l'\n",
    "\n",
    "# we ignore the later \"results\" objects, which you must obtain for yourself\n",
    "(events_sig_train, events_sig_val), (events_bkg_train, events_bkg_val) = carl.utils.load_data(data_dir, 'sig_over_bkg')\n",
    "(events_sbi_train, events_sbi_val), _, = carl.utils.load_data(data_dir, 'sbi_over_bkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd005e5b",
   "metadata": {},
   "source": [
    "### 1.(a) Scale the features\n",
    "\n",
    "The first \"usual\" thing to do is to scale the features to have $\\left< x \\right> = 0$ and standard deviation $\\sigma_x = 1$, referred to as standard scaling. Perform the following:\n",
    "\n",
    "1. Scale the features of the *training* data such that the above holds exactly.\n",
    "2. Scale the features of *validation* data exactly according to the scaling performed to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScale()\n",
    "\n",
    "features_4l = ['l1_pt', 'l1_eta', 'l1_phi', 'l1_energy', 'l2_pt', 'l2_eta', 'l2_phi', 'l2_energy', 'l3_pt', 'l3_eta', 'l3_phi', 'l3_energy', 'l4_pt', 'l4_eta', 'l4_phi', 'l4_energy']\n",
    "\n",
    "# IMPLEMENT ME\n",
    "X_sig_train = scaler.fit_transform(events_sig_train.kinematics[features_4l].to_numpy())\n",
    "X_sig_val = scaler.transform(events_sig_val.kinematics[features_4l].to_numpy())\n",
    "\n",
    "X_sbi_train = scaler.fit_transform(events_sbi_train.kinematics[features_4l].to_numpy())\n",
    "X_sbi_val = scaler.transform(events_sbi_val.kinematics[features_4l].to_numpy())\n",
    "\n",
    "X_bkg_train = scaler.fit_transform(events_bkg_train.kinematics[features_4l].to_numpy())\n",
    "X_bkg_val = scaler.transform(events_bkg_val.kinematics[features_4l].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d21de",
   "metadata": {},
   "source": [
    "### 1.(b) \"Balance\" the hypotheses\n",
    "\n",
    "The key to the likelihood ratio trick is to ensure that the neural network sees examples of two hypotheses that are balanced, i.e. their total rate of occurences in the training data are the same.\n",
    "\n",
    "$$\n",
    "    N(y = 0) = N(y = 1)\n",
    "$$\n",
    "\n",
    "For $N = 1$, then of course the neural networks only sees the relative rate, i.e. probability, of each event under the different hypotheses. Let's enforce these for each of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9364271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT ME\n",
    "w_sig_train, w_sig_val = events_sig_train.weights / events_sig_train.weights.sum(), events_sig_val.weights / events_sig_val.weights.sum()\n",
    "w_sbi_train, w_sbi_val = events_sbi_train.weights / events_sbi_train.weights.sum(), events_sbi_val.weights / events_sbi_val.weights.sum()\n",
    "w_bkg_train, w_bkg_val = events_bkg_train.weights / events_bkg_train.weights.sum(), events_bkg_val.weights / events_bkg_val.weights.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db379364",
   "metadata": {},
   "source": [
    "## 2. Building the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67452be",
   "metadata": {},
   "source": [
    "## 2. NN architecture\n",
    "\n",
    "Implement the function to specify the layers of a multi-layer perceptron (MLP) with:\n",
    "\n",
    "1. As many input nodes as there are features,\n",
    "2. As many hidden layer-times-nodes as desired, all with a ReLU activation function.\n",
    "4. One output node with a sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e4063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def nn_layers(n_features, n_layers, n_nodes):\n",
    "    # IMPLEMENT ME\n",
    "    layers = []\n",
    "    layers.append(nn.Sequential(nn.Linear(n_features, n_nodes), nn.ReLU()))\n",
    "    for _ in range(n_layers):\n",
    "        layers.append(nn.Sequential(nn.Linear(n_nodes, n_nodes), nn.ReLU()))\n",
    "    layers.append(nn.Sequential(nn.Linear(n_nodes, 1), nn.Sigmoid()))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c07b9",
   "metadata": {},
   "source": [
    "### 2.(b) Custom loss function\n",
    "\n",
    "We wish to perform a classification between the numerator $(y = 1)$ and denominator $(y = 0)$ events. Recall that we have the (balanced) weight of each event, rather than an example being an occurrence of $1$. Define a custom binary cross-entropy (BCE) function that is the weighted average accounting for the weight of each event:\n",
    "\n",
    "$$\n",
    "l_i (f(x_i), y_i) = -w_i (y_i \\log (f(x_i)) + (1-y_i) \\log(1-f(x_i)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c06870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary_cross_entropy(yhat, y):\n",
    "  # IMPLEMENT ME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919ad9c",
   "metadata": {},
   "source": [
    "Of course a way to specify this is pre-availble within the `torch` library, which will be what is used in the end. But we do this to make sure we understand what we are doing ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17104b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e54e13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
