{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36487a58-5b8f-4180-b43e-85adc2f9d556",
   "metadata": {},
   "source": [
    "# Pruning\n",
    "\n",
    "We will practice pruning using `tensorflow_model_optimization` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc37c6c7-27a7-47b9-a301-7b686c742d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27270d43-c955-4e83-89b4-c918c6c61b11",
   "metadata": {},
   "source": [
    "## Load the jet tagging dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b5f9c3-98eb-4f3e-855f-014b85396736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_val = np.load('X_train_val.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train_val = np.load('y_train_val.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "classes = np.load('classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90edffdc-7cf5-4f54-9834-ab2392ccf01c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6a24c-1316-4533-9f2d-dc36edf8c924",
   "metadata": {},
   "source": [
    "## Load the pre-trained model\n",
    "The model is trained with jet-tagging open datase: `hls4ml_lhc_jets_hlf`.\n",
    "\n",
    "We used 3 hidden layers with 64, then 32, then 32 neurons. Each layer will use relu activation. Add an output layer with 5 neurons (one for each class), then finish with Softmax activation.\n",
    "\n",
    "Look at the **training notebook** to see the details:\n",
    "https://github.com/ml4fp/2024-lbnl/blob/main/efficient_ml/part1_karas_training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26959252-404f-4cf6-82e8-f3233843b244",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 16:06:18.523277: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-08 16:06:19.029893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79047 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:82:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "model = load_model('baseline_model/KERAS_check_best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2210a79f-1494-4645-a3fc-9053e88130a4",
   "metadata": {},
   "source": [
    "## Train the pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f06d397f-a934-4586-9884-c2d6e0fac2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow_model_optimization in /global/u2/j/jcwebb/.local/perlmutter/tensorflow2.9.0/lib/python3.9/site-packages (0.8.0)\n",
      "Requirement already satisfied: absl-py~=1.2 in /global/u2/j/jcwebb/.local/perlmutter/tensorflow2.9.0/lib/python3.9/site-packages (from tensorflow_model_optimization) (1.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /global/common/software/nersc9/tensorflow/2.9.0/lib/python3.9/site-packages (from tensorflow_model_optimization) (0.1.8)\n",
      "Requirement already satisfied: numpy~=1.23 in /global/common/software/nersc9/tensorflow/2.9.0/lib/python3.9/site-packages (from tensorflow_model_optimization) (1.26.4)\n",
      "Requirement already satisfied: six~=1.14 in /global/common/software/nersc9/tensorflow/2.9.0/lib/python3.9/site-packages (from tensorflow_model_optimization) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_model_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1dc6f2b-e7d2-4231-a3d8-e434eb0fd9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "\n",
    "pruned_model = Sequential()\n",
    "pruned_model.add(Dense(64, input_shape=(16,), name=\"fc1\", kernel_initializer=\"lecun_uniform\", kernel_regularizer=l1(0.0001)))\n",
    "pruned_model.add(Activation(activation=\"relu\", name=\"relu1\"))\n",
    "pruned_model.add(Dense(32, name=\"fc2\", kernel_initializer=\"lecun_uniform\", kernel_regularizer=l1(0.0001)))\n",
    "pruned_model.add(Activation(activation=\"relu\", name=\"relu2\"))\n",
    "pruned_model.add(Dense(32, name=\"fc3\", kernel_initializer=\"lecun_uniform\", kernel_regularizer=l1(0.0001)))\n",
    "pruned_model.add(Activation(activation=\"relu\", name=\"relu3\"))\n",
    "pruned_model.add(Dense(5, name=\"output\", kernel_initializer=\"lecun_uniform\", kernel_regularizer=l1(0.0001)))\n",
    "pruned_model.add(Activation(activation=\"softmax\", name=\"softmax\"))\n",
    "\n",
    "pruning_params = {\"pruning_schedule\": pruning_schedule.ConstantSparsity(0.75, begin_step=2000, frequency=100)}\n",
    "pruned_model = prune.prune_low_magnitude(pruned_model, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f6f4e4-d4eb-4a24-98fe-1e02a7ab9134",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0056s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 16:06:26.231935: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.12310, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.12310, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 1: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 1: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 2: val_loss improved from 1.12310 to 0.97982, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.12310 to 0.97982, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 2: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 2: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 3: val_loss improved from 0.97982 to 0.90785, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.97982 to 0.90785, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 3: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 3: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 4: val_loss improved from 0.90785 to 0.87482, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 0.90785 to 0.87482, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 4: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 4: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.87482\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.87482\n",
      "\n",
      "Epoch 5: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 5: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.87482\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.87482\n",
      "\n",
      "Epoch 6: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 6: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.87482\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.87482\n",
      "\n",
      "Epoch 7: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 7: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 8: val_loss improved from 0.87482 to 0.86382, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 0.87482 to 0.86382, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 8: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 8: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 9: val_loss improved from 0.86382 to 0.84791, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 0.86382 to 0.84791, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 9: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 9: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 10: val_loss improved from 0.84791 to 0.83727, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 0.84791 to 0.83727, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 10: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 10: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 10: saving model to pruned_model/KERAS_check_model_epoch10.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 11: val_loss improved from 0.83727 to 0.82926, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 0.83727 to 0.82926, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 11: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 11: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 12: val_loss improved from 0.82926 to 0.82275, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 0.82926 to 0.82275, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 12: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 12: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 13: val_loss improved from 0.82275 to 0.81710, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 0.82275 to 0.81710, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 13: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 13: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 14: val_loss improved from 0.81710 to 0.81193, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 0.81710 to 0.81193, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 14: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 14: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 15: val_loss improved from 0.81193 to 0.80706, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 0.81193 to 0.80706, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 15: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 15: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 16: val_loss improved from 0.80706 to 0.80272, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 0.80706 to 0.80272, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 16: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 16: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 17: val_loss improved from 0.80272 to 0.79851, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 0.80272 to 0.79851, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 17: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 17: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 18: val_loss improved from 0.79851 to 0.79457, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.79851 to 0.79457, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 18: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 18: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 19: val_loss improved from 0.79457 to 0.79075, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.79457 to 0.79075, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 19: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 19: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 20: val_loss improved from 0.79075 to 0.78717, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.79075 to 0.78717, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 20: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 20: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 20: saving model to pruned_model/KERAS_check_model_epoch20.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 21: val_loss improved from 0.78717 to 0.78409, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.78717 to 0.78409, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 21: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 21: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 22: val_loss improved from 0.78409 to 0.78124, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.78409 to 0.78124, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 22: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 22: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 23: val_loss improved from 0.78124 to 0.77859, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.78124 to 0.77859, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 23: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 23: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 24: val_loss improved from 0.77859 to 0.77626, saving model to pruned_model/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.77859 to 0.77626, saving model to pruned_model/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 24: saving model to pruned_model/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 24: saving model to pruned_model/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "\n",
      "***callbacks***\n",
      "saving losses to pruned_model/losses.log\n",
      "\n",
      "Epoch 25: val_loss improved from 0.77626 to 0.77412, saving model to pruned_model/KERAS_check_best_model.h5\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(learning_rate=0.0001)\n",
    "pruned_model.compile(optimizer=adam, loss=[\"categorical_crossentropy\"], metrics=[\"accuracy\"])\n",
    "callbacks = all_callbacks(\n",
    "    stop_patience=1000,\n",
    "    lr_factor=0.5,\n",
    "    lr_patience=10,\n",
    "    lr_epsilon=0.000001,\n",
    "    lr_cooldown=2,\n",
    "    lr_minimum=0.0000001,\n",
    "    outputDir=\"pruned_model\",\n",
    ")\n",
    "callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "pruned_model.fit(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    batch_size=1024,\n",
    "    epochs=30,\n",
    "    validation_split=0.25,\n",
    "    shuffle=True,\n",
    "    callbacks=callbacks.callbacks,\n",
    "    verbose=0,\n",
    ")\n",
    "# Save the model again but with the pruning 'stripped' to use the regular layer types\n",
    "pruned_model = strip_pruning(pruned_model)\n",
    "pruned_model.save(\"pruned_model/model_best.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947b187e-417f-48c1-a3e0-50a003db3dd5",
   "metadata": {},
   "source": [
    "## Check sparsity\n",
    "\n",
    "Make a quick check that the model was indeed trained sparse. We’ll just make a histogram of the weights of the 1st layer, and hopefully observe a large peak in the bin containing ‘0’. Note logarithmic y axis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c2a6b-ebcb-40a2-9201-37447340022a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bins = np.arange(-2, 2, 0.04)\n",
    "w_unpruned = model.layers[0].weights[0].numpy().flatten()\n",
    "w_pruned = pruned_model.layers[0].weights[0].numpy().flatten()\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "plt.hist(w_unpruned, bins=bins, alpha=0.7, label=\"Unpruned layer 1\")\n",
    "plt.hist(w_pruned, bins=bins, alpha=0.7, label=\"Pruned layer 1\")\n",
    "\n",
    "plt.xlabel(\"Weight value\")\n",
    "plt.ylabel(\"Number of weights\")\n",
    "plt.semilogy()\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Sparsity of unpruned model layer 1: {np.sum(w_unpruned==0)*100/np.size(w_unpruned)}% zeros\")\n",
    "print(f\"Sparsity of pruned model layer 1: {np.sum(w_pruned==0)*100/np.size(w_pruned)}% zeros\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0018c-a646-4b64-9593-94f0b5c6f96d",
   "metadata": {},
   "source": [
    "## Check performance\n",
    "How does this 75% sparse model compare against the unpruned model? Let’s report the accuracy and make a ROC curve. The pruned model is shown with solid lines, the unpruned model is shown with dashed lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2892fe-6052-4551-9b22-180ba91bbbcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "unpruned_model = load_model(\"baseline_model/KERAS_check_best_model.h5\")\n",
    "\n",
    "y_ref = unpruned_model.predict(X_test, verbose=0)\n",
    "y_prune = pruned_model.predict(X_test, verbose=0)\n",
    "\n",
    "print(\"Accuracy unpruned: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_ref, axis=1))))\n",
    "print(\"Accuracy pruned:   {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_prune, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac618f-9acc-4c69-9f68-55e4b421775f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "_ = plotting.make_roc(y_test, y_ref, classes)\n",
    "plt.gca().set_prop_cycle(None)  # reset the colors\n",
    "_ = plotting.make_roc(y_test, y_prune, classes, linestyle=\"--\")\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "lines = [Line2D([0], [0], ls=\"-\"), Line2D([0], [0], ls=\"--\")]\n",
    "from matplotlib.legend import Legend\n",
    "\n",
    "leg = Legend(ax, lines, labels=[\"Unpruned\", \"Pruned\"], loc=\"lower right\", frameon=False)\n",
    "ax.add_artist(leg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1047f3eb-4c04-4041-9396-0f6b0d22e778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.9.0",
   "language": "python",
   "name": "tensorflow-2.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
