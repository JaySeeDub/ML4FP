{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2285437c-72ac-42c7-a70e-655b8339ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import load_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33ea9e-e41d-4dc8-ae2e-340089f9924e",
   "metadata": {},
   "source": [
    "### Let's open the training and validation files containing examples for top quarks (signal) and QCD jets (background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bbb048-dc2f-4f03-9f02-f3f4e53fb9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/pscratch/sd/v/vmikuni/datasets'\n",
    "train_data = load_data('top',input_folder,batch=128,dataset_type='train',num_evt = 100_000)\n",
    "val_data = load_data('top',input_folder,batch=128,dataset_type='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aeec21c-cfe9-47e3-9250-e14724fddce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 781 batches of events for training and 3148 for validation\n"
     ]
    }
   ],
   "source": [
    "print (f\"Loading {len(train_data)} batches of events for training and {len(val_data)} for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38a93c-14f3-49ec-90c7-65baeb1a10c8",
   "metadata": {},
   "source": [
    "### We Now need to create a model that will take the data as input and predict a label for each data entry. Let's create a config file with the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62ed545-402a-4b3d-aa49-60c3926efd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_layers': 2,\n",
    "    'hidden_dim': 64,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704f5a1f-61d6-4882-a80a-008e61a8ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.k = nn.Linear(dim, dim)\n",
    "        self.v = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, L, C = x.shape\n",
    "        \n",
    "        q = self.q(x)*mask\n",
    "        k = self.k(x)*mask\n",
    "        v = self.v(x)*mask\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) #Matrix multiplication: (B, L, C) x (B, C, L) = (B, L, L) shape\n",
    "        attn = attn.softmax(dim=-1) #Normalization\n",
    "        x = (attn @ v) # Matrix multiplication: (B, L, L) x (B, L, C) = (B, L, C)\n",
    "\n",
    "        return x*mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2a3539-2431-4f5a-8910-f399b4920d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, config, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, config[\"hidden_dim\"])\n",
    "        \n",
    "        layers = []\n",
    "        for _ in range(config[\"num_layers\"]):\n",
    "            layers.append(Attention(config[\"hidden_dim\"]))\n",
    "        self.hidden_layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(config[\"hidden_dim\"], num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        zero_pad_mask = (inputs[:, :, 2] != 0).unsqueeze(-1).float()\n",
    "        x = self.input_layer(inputs) * zero_pad_mask\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x,zero_pad_mask)\n",
    "        x = x.mean(1)  # aggregate over particles\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a778d5c8-5f18-401b-b944-38422d1164ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleTransformer(input_dim=4,config=config) #remember the inputs are delta eta, delta phi, log(pT), log(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f95bf4-8f95-4742-aa4b-dc6f3e211511",
   "metadata": {},
   "source": [
    "### Now we are going to create the training class that will train the model, but first, let's set up the learning rate and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73397f76-9973-4bad-9bf7-146a01408397",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam\n",
    "lr = 5e-4\n",
    "epochs = 100\n",
    "patience = 10 # Number of consecutive epochs to stop the training if the validation loss does not improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6c7534-3b5f-4d2d-b53c-3f17389a9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = utils.Trainer(train_data,val_data,model,lr,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea3830-5f46-45eb-b27c-0478c17451de",
   "metadata": {},
   "source": [
    "### Let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bb53f3c-c01d-4d0c-a16f-28fb5704a609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss=0.5040, validation loss=0.4716\n",
      "Epoch 2: train loss=0.4326, validation loss=0.4017\n",
      "Epoch 3: train loss=0.4013, validation loss=0.4350\n",
      "Epoch 4: train loss=0.3907, validation loss=0.3786\n",
      "Epoch 5: train loss=0.3861, validation loss=0.3783\n",
      "Epoch 6: train loss=0.3806, validation loss=0.3761\n",
      "Epoch 7: train loss=0.3759, validation loss=0.3762\n",
      "Epoch 8: train loss=0.3732, validation loss=0.3622\n",
      "Epoch 9: train loss=0.3681, validation loss=0.3753\n",
      "Epoch 10: train loss=0.3669, validation loss=0.3672\n",
      "Epoch 11: train loss=0.3662, validation loss=0.3643\n",
      "Epoch 12: train loss=0.3656, validation loss=0.3565\n",
      "Epoch 13: train loss=0.3635, validation loss=0.3577\n",
      "Epoch 14: train loss=0.3607, validation loss=0.3628\n",
      "Epoch 15: train loss=0.3595, validation loss=0.3741\n",
      "Epoch 16: train loss=0.3587, validation loss=0.3536\n",
      "Epoch 17: train loss=0.3571, validation loss=0.3560\n",
      "Epoch 18: train loss=0.3581, validation loss=0.3597\n",
      "Epoch 19: train loss=0.3592, validation loss=0.3697\n",
      "Epoch 20: train loss=0.3578, validation loss=0.3530\n",
      "Epoch 21: train loss=0.3546, validation loss=0.3618\n",
      "Epoch 22: train loss=0.3552, validation loss=0.3615\n",
      "Epoch 23: train loss=0.3552, validation loss=0.3532\n",
      "Epoch 24: train loss=0.3550, validation loss=0.3496\n",
      "Epoch 25: train loss=0.3529, validation loss=0.3577\n",
      "Epoch 26: train loss=0.3537, validation loss=0.3648\n",
      "Epoch 27: train loss=0.3541, validation loss=0.3478\n",
      "Epoch 28: train loss=0.3549, validation loss=0.3607\n",
      "Epoch 29: train loss=0.3560, validation loss=0.3592\n",
      "Epoch 30: train loss=0.3531, validation loss=0.3504\n",
      "Epoch 31: train loss=0.3544, validation loss=0.3516\n",
      "Epoch 32: train loss=0.3532, validation loss=0.3546\n",
      "Epoch 33: train loss=0.3533, validation loss=0.3502\n",
      "Epoch 34: train loss=0.3529, validation loss=0.3524\n",
      "Epoch 35: train loss=0.3527, validation loss=0.3496\n",
      "Epoch 36: train loss=0.3527, validation loss=0.3519\n",
      "Epoch 37: train loss=0.3528, validation loss=0.3504\n",
      "No improvement for 10 epochs. Early stopping at epoch 37.\n",
      "Training complete. Total time: 475.8s.\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc16252-f4ae-4027-88ef-23b258ee0ffd",
   "metadata": {},
   "source": [
    "### Now let's evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "256fe45c-70dc-46b6-a9db-1a8661a8bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_data('top',input_folder,batch=128,dataset_type='test')\n",
    "predictions, labels = trainer.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6737db7-d095-4762-a2fd-2b38aec08642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9195\n",
      "\n",
      "ACC: 0.8705\n",
      "\n",
      "Signal class 1 vs Background class 0:\n",
      "Class 1 effS at 0.30001682568589416 1.0/effB = 28.1074759849645\n",
      "Class 1 effS at 0.5000049487311453 1.0/effB = 16.49746690635725\n"
     ]
    }
   ],
   "source": [
    "utils.print_metrics(predictions,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4565450-45ce-4b2e-b5ec-0fb605a44fc3",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"vertical-align: top; padding-right: 20px;\">\n",
    "\n",
    "### Wait, why is it worse than the DeepSets model?\n",
    "A: Although attention is essential to capture correlations, the standard Transformer Architecture we use also combines additional ingredients such as linear transformations, normalization, and skip connections. These additional operations make the architecture more expressive and more stable.\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"transformer.webp\" alt=\"Transformer Block\" width=\"300\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69b43c46-c082-4669-9669-df9cd769a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.att = Attention(dim)\n",
    "        self.proj1 = nn.Linear(dim, dim)\n",
    "        self.proj2 = nn.Linear(dim, dim)\n",
    "        self.activation = nn.GELU()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.att(self.norm1(x),mask) # Attention on normalized inputs is added to the inputs\n",
    "        x = self.activation(self.proj1(x))*mask #Add a linear layer + non-linear activation        \n",
    "        x = x + self.proj2(self.norm2(x))*mask #Add another linear layer on normalized inputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "789fff44-b449-442c-a607-b7dcd654935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, config, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, config[\"hidden_dim\"])\n",
    "        \n",
    "        layers = []\n",
    "        for _ in range(config[\"num_layers\"]):\n",
    "            layers.append(TransformerBlock(config[\"hidden_dim\"]))\n",
    "        self.hidden_layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(config[\"hidden_dim\"], num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        zero_pad_mask = (inputs[:, :, 2] != 0).unsqueeze(-1).float()\n",
    "        x = self.input_layer(inputs) * zero_pad_mask\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x,zero_pad_mask)\n",
    "        x = x.mean(1)  # aggregate over particles\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eef8519-4b95-486a-9bbd-ec1baafcda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(input_dim=4,config=config) \n",
    "optimizer = torch.optim.Adam\n",
    "lr = 5e-4\n",
    "epochs = 100\n",
    "patience = 10 # Number of consecutive epochs to stop the training if the validation loss does not improve\n",
    "trainer = utils.Trainer(train_data,val_data,model,lr,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1fe0bae-7429-4398-ace5-db793e47c198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss=0.3955, validation loss=0.2966\n",
      "Epoch 2: train loss=0.2931, validation loss=0.2758\n",
      "Epoch 3: train loss=0.2770, validation loss=0.2835\n",
      "Epoch 4: train loss=0.2589, validation loss=0.2429\n",
      "Epoch 5: train loss=0.2483, validation loss=0.2440\n",
      "Epoch 6: train loss=0.2392, validation loss=0.2304\n",
      "Epoch 7: train loss=0.2311, validation loss=0.2458\n",
      "Epoch 8: train loss=0.2279, validation loss=0.2371\n",
      "Epoch 9: train loss=0.2232, validation loss=0.2238\n",
      "Epoch 10: train loss=0.2210, validation loss=0.2144\n",
      "Epoch 11: train loss=0.2181, validation loss=0.2212\n",
      "Epoch 12: train loss=0.2160, validation loss=0.2685\n",
      "Epoch 13: train loss=0.2144, validation loss=0.2161\n",
      "Epoch 14: train loss=0.2128, validation loss=0.2119\n",
      "Epoch 15: train loss=0.2093, validation loss=0.2086\n",
      "Epoch 16: train loss=0.2085, validation loss=0.2221\n",
      "Epoch 17: train loss=0.2055, validation loss=0.2133\n",
      "Epoch 18: train loss=0.2047, validation loss=0.2090\n",
      "Epoch 19: train loss=0.2026, validation loss=0.2021\n",
      "Epoch 20: train loss=0.2022, validation loss=0.2055\n",
      "Epoch 21: train loss=0.1998, validation loss=0.2031\n",
      "Epoch 22: train loss=0.1985, validation loss=0.2158\n",
      "Epoch 23: train loss=0.1985, validation loss=0.2096\n",
      "Epoch 24: train loss=0.1972, validation loss=0.2034\n",
      "Epoch 25: train loss=0.1964, validation loss=0.2047\n",
      "Epoch 26: train loss=0.1960, validation loss=0.1981\n",
      "Epoch 27: train loss=0.1950, validation loss=0.2024\n",
      "Epoch 28: train loss=0.1944, validation loss=0.2044\n",
      "Epoch 29: train loss=0.1945, validation loss=0.1982\n",
      "Epoch 30: train loss=0.1932, validation loss=0.1970\n",
      "Epoch 31: train loss=0.1913, validation loss=0.2012\n",
      "Epoch 32: train loss=0.1919, validation loss=0.2028\n",
      "Epoch 33: train loss=0.1910, validation loss=0.1968\n",
      "Epoch 34: train loss=0.1892, validation loss=0.2012\n",
      "Epoch 35: train loss=0.1899, validation loss=0.2014\n",
      "Epoch 36: train loss=0.1894, validation loss=0.1959\n",
      "Epoch 37: train loss=0.1890, validation loss=0.1936\n",
      "Epoch 38: train loss=0.1886, validation loss=0.1987\n",
      "Epoch 39: train loss=0.1887, validation loss=0.1975\n",
      "Epoch 40: train loss=0.1873, validation loss=0.1963\n",
      "Epoch 41: train loss=0.1864, validation loss=0.1935\n",
      "Epoch 42: train loss=0.1859, validation loss=0.1939\n",
      "Epoch 43: train loss=0.1860, validation loss=0.2001\n",
      "Epoch 44: train loss=0.1854, validation loss=0.1937\n",
      "Epoch 45: train loss=0.1852, validation loss=0.1957\n",
      "Epoch 46: train loss=0.1851, validation loss=0.1978\n",
      "Epoch 47: train loss=0.1836, validation loss=0.1952\n",
      "Epoch 48: train loss=0.1835, validation loss=0.1942\n",
      "Epoch 49: train loss=0.1836, validation loss=0.2137\n",
      "Epoch 50: train loss=0.1824, validation loss=0.2021\n",
      "Epoch 51: train loss=0.1825, validation loss=0.1967\n",
      "No improvement for 10 epochs. Early stopping at epoch 51.\n",
      "Training complete. Total time: 698.3s.\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a3f5ed7-2f4b-478f-be48-ac20e8cb375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels = trainer.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c509579-16cb-4d7d-ab3b-aacaccc69082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9758\n",
      "\n",
      "ACC: 0.9228\n",
      "\n",
      "Signal class 1 vs Background class 0:\n",
      "Class 1 effS at 0.30002127985984983 1.0/effB = 337.623745819398\n",
      "Class 1 effS at 0.5000371160346219 1.0/effB = 117.58823529411764\n"
     ]
    }
   ],
   "source": [
    "utils.print_metrics(predictions,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c1c41-0f8f-4fdf-897f-81f4ed0da18d",
   "metadata": {},
   "source": [
    "### Try changing the hyperparameters of the model to see if you can improve the results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.6.0",
   "language": "python",
   "name": "pytorch-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
