{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-DStRWBSe_3"
   },
   "source": [
    "# Generative models exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This exercise is based on a normalizing flow exercise designed by T.Quadfasel, M.Sommerhalder and S.Diefenbacher, https://github.com/uhh-pd-ml/flow-exercise\n",
    "\n",
    "In order to run the exercise a special environment is needed. In a terminal run the following 3 lines:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "module load python\n",
    "\n",
    "source activate /global/common/software/trn016/python_envs/ml4fp2025Day4\n",
    "\n",
    "python -m ipykernel install --user --name training_env_pytorch --display-name \"ML4FP2025_Day4\"\n",
    "```\n",
    "Then swap the kernel of this notebook to 'ML4FP2025_Day4'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Moons Data Set\n",
    "\n",
    "One common example used for benchmarking is the so-called 'two moon' dataset, consisting of two interlocking half circles. On the one hand, it tests the generative model's ability to replicate a complex structure, on the other hand, this data set also features two separate subsets, which our generative model will have to keep separate as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJyPtwUJSfAH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we will get the dataset from scikit-learn\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# for two moons dataset\n",
    "\n",
    "\n",
    "# define normalization function\n",
    "def normalize_moons(in_data):\n",
    "    max_val = np.max(in_data, keepdims=True, axis=0)\n",
    "    min_val = np.min(in_data, keepdims=True, axis=0)\n",
    "\n",
    "    new_data = (in_data - min_val) / (max_val - min_val)\n",
    "    mask = np.prod(((new_data < 1) & (new_data > 0)), axis=1, dtype=bool)\n",
    "    new_data = new_data[mask]\n",
    "    return new_data, mask\n",
    "\n",
    "\n",
    "n = 10000\n",
    "X_moons, _ = datasets.make_moons(n_samples=n, noise=0.05)\n",
    "X_moons, _ = normalize_moons(X_moons)\n",
    "\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], color='black', marker='.', linewidth=0)\n",
    "plt.savefig(\"./original_two_moons.pdf\")\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "In our next step, we import the PyTorch package, which we will be using as our machine learning backend. We also implement a generic fully connected neural network class, which we will be using in the following generative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# our usual PyTorch functionality\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# General purpose dense net\n",
    "class DenseNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, n_layers, activation_fn=F.relu, last_activation_fn=None):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(torch.nn.Linear(self.input_dim, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            self.layers.append(torch.nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "\n",
    "        self.layers.append(torch.nn.Linear(self.hidden_dim, self.output_dim))\n",
    "\n",
    "        self.activation_fn = activation_fn\n",
    "        self.last_activation_fn = last_activation_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = self.activation_fn(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        if self.last_activation_fn is not None:\n",
    "            x = self.last_activation_fn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Variational AutoEncoder\n",
    "\n",
    "The Variational Autoencoder implementation shown here explicitly defines the encoder and decoder models as two separate dense networks. The Endocer input and Decoder output have to match the dimensionality of the data set. The  Endocer output and Decoder input define the latent space their size is arbitrary, however, they need to be consistent such that the Encoder output is twice as large as the Decoder input. This is due to the fact in order to produce the Gaussian latent space, the Encoder needs to output a width and a mean for each Decoder input.\n",
    "\n",
    "In principle, this specific VAE is a Beta-VAE, as it has an explicit scaling parameter between the MSE reconstruction loss and the KLD latent regularization loss. However, for a beta parameter of 1, this setup is identical to a standard VAE. \n",
    "\n",
    "**Task: The VEA implementation below is functional, but far from optimal. Run the subsequent two cells, and observe the Two Moons samples produced by the generator. Then modify some of the marked parameters and observe if the result improves. Especially the Beta parameter is interesting to vary**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "### Modifiy network parameters here ###\n",
    "EncoderNet = DenseNet(input_dim=2, output_dim=4, hidden_dim=32, n_layers=4).to(device)\n",
    "DecoderNet = DenseNet(input_dim=2, output_dim=2, hidden_dim=32, n_layers=4).to(device)\n",
    "#######################################\n",
    "\n",
    "\n",
    "EncoderOpt = optim.Adam(EncoderNet.parameters(), lr=1e-5)\n",
    "DecoderOpt = optim.Adam(DecoderNet.parameters(), lr=1e-5)\n",
    "\n",
    "#######################################\n",
    "### modify training parameters here ###\n",
    "batch_size = 200\n",
    "epochs = 100\n",
    "#######################################\n",
    "\n",
    "###################################\n",
    "### modify loss parameters here ###\n",
    "beta = 1\n",
    "###################################\n",
    "\n",
    "max_batches = int(X_moons.shape[0] / batch_size)\n",
    "\n",
    "MSEloss = nn.MSELoss()\n",
    "for ep in range(epochs):\n",
    "    for i_batch in range(max_batches):\n",
    "        EncoderOpt.zero_grad()\n",
    "        DecoderOpt.zero_grad()\n",
    "\n",
    "        # select the current batch from the dataset\n",
    "        x_real = X_moons[i_batch * batch_size : (i_batch + 1) * batch_size]\n",
    "        x_real = torch.tensor(x_real, device=device).float()\n",
    "\n",
    "        latent = EncoderNet(x_real)\n",
    "        mu = latent[:, ::2]\n",
    "        log_var = latent[:, 1::2]\n",
    "\n",
    "        KLD = torch.mean(-0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0)\n",
    "\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std, device=device)\n",
    "        reparameterized = eps * std + mu\n",
    "\n",
    "        x_recon = DecoderNet(reparameterized)\n",
    "\n",
    "        MSE = MSEloss(x_real, x_recon)\n",
    "\n",
    "        loss = KLD * beta + MSE\n",
    "        loss.backward()\n",
    "\n",
    "        EncoderOpt.step()\n",
    "        DecoderOpt.step()\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        print(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's use the fitted distribution and sample from it\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn((1000, 2), device=device).float()\n",
    "    samples = DecoderNet(noise).cpu().numpy()\n",
    "\n",
    "\n",
    "# plot the results\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], color='black', marker='.', linewidth=0)\n",
    "plt.scatter(samples[:, 0], samples[:, 1], color='darkblue', marker='o', linewidth=0)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbNI8qfkGSm8"
   },
   "source": [
    "# 2 Diffusion Models\n",
    "\n",
    "## 2a.) Introduction\n",
    "\n",
    "Much like normalizing flows, diffusion models are designed to learn the inverse of mapping a data point to random noise. The math underpinning diffusion models is ironically significantly more complex than the math required to train and run a diffusion model. \n",
    "\n",
    "Therefore, this section will focus on how to train a basic diffusion model, without performing the first principle derivations.\n",
    "\n",
    "The fundamental idea behind diffusion models is to take a data point, repeatedly apply a Gaussian noise kernel to it, and then learn to undo that Gaussian smearing. In practice, repeatedly applying smearing is done using a Gaussian kernel with properties derived from the noise schedule beta. \n",
    "This emulates the effect of multiple smearings and broadly follows the original denoising diffusion papers, e.g. https://arxiv.org/abs/2006.11239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "timesteps = 10\n",
    "beta = torch.linspace(start=1e-4, end=1e-1, steps=timesteps).to(device)\n",
    "\n",
    "# define alpha for forward diffusion kernel\n",
    "alpha = 1 - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us plot the noise function to see how the data looks, with added noise, after a given timestep:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for step in range(timesteps-1, -1, -1):\n",
    "    \n",
    "    sample = torch.tensor(X_moons, device=device).float()\n",
    "    noise = torch.randn_like(sample) \n",
    "\n",
    "    noisy_sample = (sample * torch.sqrt(alpha_bar[step]) + \n",
    "                    noise * torch.sqrt(1-alpha_bar[step]))\n",
    "    \n",
    "    samples = noisy_sample.cpu().numpy()\n",
    "   \n",
    "    plt.scatter(samples[:, 0], samples[:, 1], color='darkblue', marker='o', linewidth=0)\n",
    "    plt.scatter(X_moons[:, 0], X_moons[:, 1], color='black', marker='.', linewidth=0)\n",
    "    plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to allow the denoising model to reduce Gaussian smearings with different severity, it needs to be conditioned on the current timestep to be denoised. However, the difference between two timesteps is numerically speaking tiny, and the network may struggle to pick up the small differences between timesteps. \n",
    "\n",
    "Therefore we make use of Sinusoidal embedding. This translates small changes in the timestep into unique combinations of sin and cosine evaluations, based on the given timestep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_SinusoidalPosEmb = True\n",
    "\n",
    "import math\n",
    "def SinusoidalPosEmb(x, dim):\n",
    "    device = x.device\n",
    "    half_dim = dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "    emb = x[:, None] * emb[None, :]\n",
    "    emb = torch.cat((emb.sin(), emb.cos()), dim=-1).view(-1,dim) \n",
    "    return emb\n",
    "\n",
    "if use_SinusoidalPosEmb:\n",
    "    embed_dim = 16\n",
    "    embed_func = lambda x: SinusoidalPosEmb(x, embed_dim)\n",
    "else:    \n",
    "    embed_dim = 1\n",
    "    embed_func = lambda x: x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the diffusion model. To this end, we apply a smearing for a randomly determined timestep and then learn to undo this smearing. The loss in this case is a simple mean squared error between the applied smearing and the inverse smearing predicted by the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######\n",
    "# Modifications here\n",
    "timesteps = 10\n",
    "beta = torch.linspace(start=1e-4, end=1e-1, steps=timesteps).to(device) \n",
    "######\n",
    "\n",
    "\n",
    "# define alpha for forward diffusion kernel\n",
    "alpha = 1 - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "\n",
    "\n",
    "######\n",
    "# Modifications here\n",
    "DenoiseModel = DenseNet(input_dim=2+embed_dim, output_dim=2, hidden_dim=256, n_layers=4).to(device)\n",
    "\n",
    "DenoiseOpt = optim.Adam(DenoiseModel.parameters(), lr=1e-4)\n",
    "\n",
    "batch_size = 200\n",
    "epochs = 10\n",
    "######\n",
    "\n",
    "\n",
    "max_batches = int(X_moons.shape[0] / batch_size)\n",
    "\n",
    "MSEloss = nn.MSELoss()\n",
    "for ep in range(epochs):\n",
    "    ep_loss = 0\n",
    "    for i_batch in range(max_batches):\n",
    "        # select the current batch from the dataset\n",
    "        sample = X_moons[i_batch * batch_size : (i_batch + 1) * batch_size]\n",
    "        sample = torch.tensor(sample, device=device).float()\n",
    "\n",
    "        DenoiseOpt.zero_grad()\n",
    "        \n",
    "        step = np.random.randint(0, timesteps)\n",
    "        step_network_input = torch.ones(sample.shape[0], 1, device=device)*step\n",
    "        step_network_input = embed_func(step_network_input)\n",
    "        \n",
    "        \n",
    "        noise = torch.randn_like(sample) \n",
    "\n",
    "        noisy_sample = (sample * torch.sqrt(alpha_bar[step]) + \n",
    "                        noise * torch.sqrt(1-alpha_bar[step]))\n",
    "\n",
    "        noise_pred = DenoiseModel(torch.cat([noisy_sample, step_network_input], -1))\n",
    "        \n",
    "        loss = MSEloss(noise, noise_pred)\n",
    "        ep_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        DenoiseOpt.step()\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        print(ep)\n",
    "        print(ep_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we apply the model to a random gaussian for each timestep to achieve a two-moons-like sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_sample = 2000\n",
    "\n",
    "noisy_sample = torch.randn((n_sample, X_moons.shape[-1])).to(device)\n",
    "\n",
    "for t in range(timesteps-1, -1, -1):\n",
    "    \n",
    "    if t > 1:\n",
    "        z = torch.randn_like(noisy_sample).to(device)\n",
    "    elif t==0:\n",
    "        z = torch.zeros_like(noisy_sample).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        step_network_input = torch.ones(noisy_sample.shape[0], 1, device=device)*t\n",
    "        step_network_input = embed_func(step_network_input)\n",
    "\n",
    "        noise_pred = DenoiseModel(torch.cat([noisy_sample, step_network_input], -1))\n",
    "\n",
    "    # denoise at time t, utilizing predicted noise\n",
    "    noisy_sample_new = ((1 / torch.sqrt(alpha[t])) * \n",
    "                        (noisy_sample - (1-alpha[t])/torch.sqrt(1-alpha_bar[t])*noise_pred) + \n",
    "                        torch.sqrt(beta[t])*z)\n",
    "\n",
    "    noisy_sample = noisy_sample_new\n",
    "    \n",
    "\n",
    "samples = noisy_sample\n",
    "samples = samples.cpu().numpy()\n",
    "\n",
    "# plot the results\n",
    "plt.scatter(samples[:, 0], samples[:, 1], color='darkblue', marker='o', linewidth=0)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], color='black', marker='.', linewidth=0)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Task: Improve the performance of the network. Tip: Modify the noise schedule. Consider using more timesteps and maybe even a different, non-linear shape**\n",
    "\n",
    "\n",
    "**Bonus Task:\n",
    "Modify the sampling code above to show the intermediated generation steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHlmT4cuSe_-"
   },
   "source": [
    "# 3 Normalizing Flows\n",
    "\n",
    "## 3a.) Motivation\n",
    "\n",
    "Just as Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs), Normalizing Flows are **Probabilistic Generative Models (PGMs)**, which describe a probability distribution that we attempt to learn from a set of observed data. \n",
    "\n",
    "PGMs are useful for generating new samples from the learned distribution, evaluating the likelihood of new data points, etc. \n",
    "\n",
    "Normalizing flows are PGMs built on **invertible transformations**. Their advantages are that it is typically possible to efficiently **sample** and **evaluate** the learned distributions. Normalizing flows also are **highly expressive** and come with a **useful latent space representation**, since we have a one-to-one mapping between our input and the latent space. Finally, they are also **easy to train**, since we just need to conduct a simple maximum likelihood training.\n",
    "\n",
    "In this hands-on exercise, we will show examples of using normalizing flows to estimate (conditional) probability densities and generate new samples. While some instructive mock datasets will be used for getting to know Normalizing Flows, we will also have one dedicated exercise that will use a Particle Physics dataset in the end. As a framework for actually implementing Normalizing Flows quickly, we will use the **nFlows** package. We hope this tutorial will give you at least a rough idea of how Normalizing Flows work in practice and - most importantly - that you find it instructive and fun :)\n",
    "\n",
    "## 3b.) Normalizing Flows - The general idea (recap)\n",
    "\n",
    "At the heart of normalizing flows stands the change of variables formula. Let's say we had a random variable $U$ and now, we apply a simple transformation, defining the transformed random variable as $X$. Then, the change of variable formula is given by:\n",
    "\n",
    "$$ p(x) = p(u)\\left| \\frac{df(u)}{du}\\right|^{-1} $$\n",
    "\n",
    "As an example, let's have a look at the figure below:\n",
    "\n",
    "<br />\n",
    "<img src=\"./FlowImage1.png\" width=\"250\">\n",
    "<br />\n",
    "\n",
    "We start with a uniform distribution $U$ between 0 and 1, and then do a transformation $f(U)=2\\cdot U + 1$. This, however, leads to a distribution that is not normalized, so to get a proper probability distribution, we still need to scale it with $ \\left| \\left(\\frac{df(u)}{du}\\right)\\right|^{-1} $.\n",
    "\n",
    "The derivative of $f$ with respect to $u$ is 2, and the inverse is $\\frac{1}{2}$, which is exactly the scaling factor that we need for getting a properly normalized distribution.\n",
    "\n",
    "This, however, is only a simple univariate example. For the multivariate case, instead of the derivative, we need to scale our transformed distribution with the Jacobian determinant:\n",
    "\n",
    "$$ \\left|\\det \\left( \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{u}} \\right)  \\right|^{-1} = \\left|\\det \\left( \\frac{\\partial \\boldsymbol{f}^{-1}}{\\partial \\boldsymbol{x}} \\right)  \\right| $$\n",
    "\n",
    "The main idea of normalizing flows is to learn an invertible mapping between a very complex distribution (for example a distribution of a physics variable) and a very simple distribution. Let us consider the \"generative\" part first, where we attempt to generate new samples from a Normalizing Flow. In this case, we start with a simple distribution, for example, a standard Gaussian, and then repeatedly apply the random variable transformations to acquire a complex distribution similar to the input distribution that we want to approximate. Let's say we apply $k$ transformations $f_{1}...f_{k}$, starting out from a simple distribution $u_{0}$ and going to a complex distribution $u_{k}$, then the change of random variables formula becomes:\n",
    "\n",
    "$$ p(\\boldsymbol{u}_{k}) = p(\\boldsymbol{u}_{0})\\prod_{i} \\left|\\det \\left( \\frac{\\partial \\boldsymbol{f}_{i}}{\\partial \\boldsymbol{u}_{i-1}} \\right)  \\right|^{-1} $$\n",
    "\n",
    "\n",
    "From this formula, we also see the reason why this method is called \"normalizing flows\". The random variable \"flows\" through a series of transformations while staying normalized by the scaling with the Jacobian determinant.\n",
    "\n",
    "<br />\n",
    "<img src=\"./FlowImage2.png\" width=\"1000\">\n",
    "<br />\n",
    "\n",
    "In a practical case, our \"complex\" random variables $\\boldsymbol{u}_{k}$ would then approximate our input variables.\n",
    "\n",
    "Since we choose our $\\boldsymbol{f}_{i}$ to be invertible, we can also run the Normalizing Flow in the other direction, starting out with our complex input distribution and mapping this distribution to a standard Gaussian. This is the direction used for density/likelihood estimation (i.e. if you want to evaluate the likelihood of new data under the learned distribution) and also the one used for training.\n",
    "\n",
    "For training a normalizing flow, remember that our $\\boldsymbol{f}_{i}$ are actually neural networks with parameters $\\theta$. We train our flow by optimizing these parameters in a negative logarithmic likelihood minimization (which is our loss function then). So we take our input samples $\\boldsymbol{u}_{k}$, take it \"backward\" through the flow (i.e. in the direction of the blue arrows in the picture above), and then tune the parameters of our $\\boldsymbol{f}_{i}$ such that the likelihood of the so transformed distribution under a standard Gaussian gets maximized.\n",
    "\n",
    "To be able to use normalizing flows efficiently, we, therefore, need functions $f$ that are both **invertible** and **have a tractable Jacobian** that is **easy to compute**. Several methods have been proposed so far and today, you will work mainly with so-called \"autoregressive flows\", where the functions $f$ are chosen in a way such that the Jacobian is guaranteed to be an upper triangular matrix, so we can compute the determinant simply by multiplying its diagonal elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjLFuT_3Se__"
   },
   "source": [
    "## 3c.) The nFlows package \n",
    "\n",
    "Today, we are going to use the PyTorch nFlows library, which is easy to set up and already contains implementations of many of the different flow models. \n",
    "\n",
    "Details and code of the package can be found here https://github.com/bayesiains/nflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcFRcKKFA4gS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the nflows functions what we will need in order to build our flow\n",
    "from nflows.flows.base import Flow  # a container that will wrap the parts that make up a normalizing flow\n",
    "from nflows.distributions.normal import StandardNormal  # Gaussian latent space distribution\n",
    "from nflows.transforms.base import (\n",
    "    CompositeTransform,\n",
    ")  # a wrapper to stack simpler transformations to form a more complex one\n",
    "from nflows.transforms.autoregressive import (\n",
    "    MaskedAffineAutoregressiveTransform,\n",
    ")  # the basic transformation, which we will stack several times\n",
    "from nflows.transforms.autoregressive import (\n",
    "    MaskedPiecewiseRationalQuadraticAutoregressiveTransform,\n",
    ")  # the basic transformation, which we will stack several times\n",
    "from nflows.transforms.permutations import ReversePermutation  # a layer that simply reverts the order of outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJCJe_sMSfAC"
   },
   "source": [
    "Some background about the different modules:\n",
    "\n",
    "The **flows module** contains the implementation of the actual flow models. The classes in this module already implement the functions to calculate the negative log-likelihood and the jacobian determinant.\n",
    "\n",
    "Every flow model is defined as a series of transformations, contained in the **transformation module**. Here we will be using two different transformations, `MaskedAffineAutoregressiveTransform`, which provides the actual invertible transformation, and `ReversePermutation` which serves to permute the features within the flow. The final transformation `CompositeTransform` allows for the combining of several transformations into one module. \n",
    "\n",
    "Finally, the **distributions module** contains the necessary base distributions that form the latent space. In our case, we will use a gaussian latent space, defined by `StandardNormal`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The first model we will want to look at is the Masked Autoregressive Flow (MAF) https://arxiv.org/abs/1705.07057.\n",
    "\n",
    "The autoregressive property on a sequence of data $\\boldsymbol{x} = [x_1, …, x_D]$ states that each output only depends on previous data. This means that the probability of $x_i$ is conditioned only on $x_1, ... ,x_{i-1}$. The full probability density $p(\\boldsymbol{x})$ then becomes the product of the conditional densities:\n",
    "$$p(\\boldsymbol{x}) = \\Pi_{i=1}^{D}p(x_i|x_1, …, x_{i-1})$$\n",
    "\n",
    "An autoregressive flow makes use of this property in arranging the dependencies of the input dimensions. In each layer, the output $f_{i}(\\boldsymbol{x})$ only depends on features $x_1, .. ,x_{i-1}$. The reason for this is the simpler form of the jacobian determinant. The matrix of partial derivates $\\frac{\\partial \\boldsymbol{f}_{i}}{\\partial \\boldsymbol{x}_{j}}$ becomes triangular because there is no dependency between earlier input dimensions $\\frac{\\partial \\boldsymbol{f}_{i}}{\\partial \\boldsymbol{x}_{j > i}} = 0$. As a result, the determinant is simply the product of diagonal elements.\n",
    "\n",
    "The variable transformation of the MAF is a shift and scaling operation (i.e. an affine transformation):\n",
    "$$z_i = x_i \\exp(s_i(\\boldsymbol{x})) + t_i(\\boldsymbol{x})$$ \n",
    "but it chooses neural networks $\\boldsymbol{s}$ and $\\boldsymbol{t}$ such that they respect the autoregressive property $s_i(x_1, .., x_{i-1})$ and $t_i(x_1, .., x_{i-1})$. It achieves this by stacking so-called MADE blocks (https://arxiv.org/abs/1502.03509), which are sets of neural network layers that apply element-wise multiplications of mask matrices to the weights. The mask matrices simply contain ones and zeros, intending to turn off dependencies that would violate the autoregressive property. After each flow layer, one applies a permutation on the order of inputs.\n",
    "\n",
    "\n",
    "Let's now define such a simple MAF with the nflows package. In this framework, a flow consists of a base (latent space) distribution and an invertible transformation. This flow object then has two important methods: `flow.log_prob(data)` returns the logarithmic probability of the data, `flow.sample(n_samples)` produces new data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5Bx-UG7SfAC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fist we define the latent space distribution, in this case a choosing a 2-dim Gaussian\n",
    "base_dist = StandardNormal(shape=[2])\n",
    "\n",
    "# Now we define the series of transformation that our flow model will comprise.\n",
    "# For now we will use a singel MAF layer, we preemtivly implement this as a list,\n",
    "# so to make it simple to extend.\n",
    "\n",
    "# For the MAF layer we need to secify the number of input parameters, as well as the\n",
    "# number of features used in the internal FCN layers.\n",
    "transforms = []\n",
    "transforms.append(MaskedAffineAutoregressiveTransform(features=2, hidden_features=4))\n",
    "\n",
    "# Finally we can combine the list of previously defined transformations into one\n",
    "# single compisite transformation.\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "# The actual flow now consist of the the base distribution and transform together\n",
    "flow = Flow(transform, base_dist).to(device)\n",
    "\n",
    "# we can then use standard PyTorch optimizers\n",
    "optimizer = optim.Adam(flow.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6J_LURVNjNlF"
   },
   "source": [
    "# 3d.) Data Generation Using Flows\n",
    "\n",
    "One major advantage of Normalizing Flow Networks is that they are inherently invertible. This means that we can not only use them for density estimation by mapping our data set to a well-known distribution like a Normal Gaussian, but we can also do the reverse, mapping the Normal Gaussian to our data distribution. This essentially allows us to generate new data by feeding Gaussian samples backward through our model. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ly022hdvIC-8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "\n",
    "max_batches = int(X_moons.shape[0] / batch_size)\n",
    "\n",
    "for i_batch in range(max_batches):\n",
    "    # select the current batch from the dataset\n",
    "    x = X_moons[i_batch * batch_size : (i_batch + 1) * batch_size]\n",
    "    x = torch.tensor(x, device=device).float()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # calculate negative log likelihood\n",
    "    nll = -flow.log_prob(x)\n",
    "\n",
    "    # update the model\n",
    "    loss = nll.mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwoxoJXQSfAE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's use the fitted distribution and sample from it\n",
    "with torch.no_grad():\n",
    "    samples = flow.sample(1000).cpu().numpy()\n",
    "\n",
    "\n",
    "# plot the results\n",
    "plt.scatter(samples[:, 0], samples[:, 1], color='darkblue', marker='o', linewidth=0)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], color='black', marker='.', linewidth=0)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Fy5VaGgSfAE"
   },
   "outputs": [],
   "source": [
    "# Let's also plot how well our data is mapped to the Gaussian base distribution\n",
    "\n",
    "inv = flow.transform_to_noise(torch.tensor(X_moons).float().to(device)).detach().cpu().numpy()\n",
    "plt.scatter(inv[:, 0], inv[:, 1], color='black', marker='.', linewidth=0)\n",
    "plt.xlabel(\"Transformed x1\")\n",
    "plt.ylabel(\"Transformed x2\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbtlEySDSfAF"
   },
   "source": [
    "As we can see, our initial density is estimated rather poorly. We also see that our mapped data is far from being Gaussian distributed in the latent space. So what did we do wrong? We actually didn't use a proper flow! For getting to know nFLows, a single MAF transformation was used here. With the knowledge of how to use the module, we can now start building a powerful flow, consisting of multiple layers.\n",
    "\n",
    "**Task: Modify the Code below to improve the generation result**\n",
    "\n",
    "**Bonus Task: Rational Quadratic Splines (RQS) are a more complex and advanced type of invertible transformations, that see use in state-of-the-art normalizing flows. nFlows provides a prebuilt implementation of these transformations, which can be accessed via:**\n",
    "\n",
    "```\n",
    "MaskedPiecewiseRationalQuadraticAutoregressiveTransform(features=2, \n",
    "hidden_features=16, tail_bound = 3.0, tails = \"linear\"))\n",
    "```\n",
    "\n",
    "**Try replacing the MAF transformations with RQS transformations and see what effects can be observed observe.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odmPNp-hSfAF"
   },
   "source": [
    "# 3e.) Two Moons Generation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K23FrzYXSfAF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_dist = StandardNormal(shape=[2])\n",
    "\n",
    "######\n",
    "# Modifications here\n",
    "transforms = []\n",
    "transforms.append(MaskedAffineAutoregressiveTransform(features=2, hidden_features=16))\n",
    "transforms.append(ReversePermutation(features=2))\n",
    "######\n",
    "\n",
    "\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist).to(device)\n",
    "\n",
    "optimizer = optim.Adam(flow.parameters())\n",
    "\n",
    "\n",
    "######\n",
    "# Modifications here\n",
    "num_epochs = 1\n",
    "batch_size = 200\n",
    "######\n",
    "\n",
    "max_batches = int(X_moons.shape[0] / batch_size)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    permut = np.random.permutation(X_moons.shape[0])\n",
    "    X_moons_shuffle = X_moons[permut]\n",
    "    for i_batch in range(max_batches):\n",
    "        x = X_moons_shuffle[i_batch * batch_size : (i_batch + 1) * batch_size]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        nll = -flow.log_prob(x)\n",
    "\n",
    "        loss = nll.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print('Epoch: {:d}'.format(i))\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples = flow.sample(1000).cpu().numpy()\n",
    "\n",
    "# transform them like we did when plotting the original dataset\n",
    "# samples = StandardScaler().fit_transform(samples)\n",
    "\n",
    "# plot the results\n",
    "plt.scatter(samples[:, 0], samples[:, 1], color='darkblue', marker='o', linewidth=0)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], color='black', marker='.', linewidth=0)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvmCqnb-LJys"
   },
   "source": [
    "# 3f.) Intermediate transformation steps\n",
    "\n",
    "So far, our implementation of normalizing flows was very effective and easy to use, however it was also rather BlackBox-y. \n",
    "\n",
    "The code below illustrates how one can extract the individual transformations and the base distribution from the flow model, and then perform the first `n_transforms` transformations \"by hand\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFAJO-nNLUJQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of transforms to perform\n",
    "n_transforms = 1\n",
    "\n",
    "# Ensure the number of transforms to perform is lower than the total amount of transforms\n",
    "n_transforms = min(len(flow._transform._transforms), n_transforms)\n",
    "\n",
    "# Here we access the base distribution of the flow model and sample from it\n",
    "z = flow._distribution.sample(1000)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(1, n_transforms):\n",
    "        # The flow model has an attribute called '_transform', which again has an attribute\n",
    "        # '_transforms', which is a list containing the individual transforms\n",
    "        # For the purpose of generating samples we need to iterate through the transforms in\n",
    "        # reverse order, hence the negative index\n",
    "        m = flow._transform._transforms[-i]\n",
    "\n",
    "        # The transforms can be applied like any other PyTorch module, however, we need\n",
    "        # to specify the direction, in the case of sampling we need the inverse.\n",
    "        # The function has two return parameters, the transformed data, and the log-determinant\n",
    "        # for the sample generation we only care about the former and map the latter to a _\n",
    "        z, _ = m.inverse(z)\n",
    "\n",
    "\n",
    "plt.scatter(z.cpu().numpy()[:, 0], z.cpu().numpy()[:, 1], color='darkblue', marker='o', linewidth=0)\n",
    "plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsaNorE-UKoG"
   },
   "source": [
    "So far we only see the effects of the first transformation\n",
    "\n",
    "**Task: Use the above code to plot the intermediate distributions for all flow transformations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhipO0QxQHU6"
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Add code here #\n",
    "#################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOkCS4rTZBKK"
   },
   "source": [
    "## 3g.) Conditional Generative Flows\n",
    "\n",
    "Many generative tasks in particle physics require the generative model to not only generate a random point from the data set but instead generate a point with a specific property. One example is shower simulation, where a generative shower simulator should be able to produce showers for a given particle energy, rather than just for a random one. \n",
    "\n",
    "This is where conditional generative models become important. In the previous example, the energy value would be the condition given to the model. \n",
    "\n",
    "Let us try this principle on flows with our previous two-moons data set. As one can see, it is made up of two distinct subsets, (the eponymous moons). Our conditioning will be to which moon a datapoint belongs, e.g. if we tell the model to generate points with the label 0 they should be from the top moon, while points with the label 1 should be from the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BHN2hoKaDNC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# for two moons dataset\n",
    "n = 10000\n",
    "# the two moons sampling function already returns labels to which moon a data point belongs\n",
    "X_moons, label_moon = datasets.make_moons(n_samples=n, noise=0.05)\n",
    "X_moons, mask = normalize_moons(X_moons)\n",
    "label_moon = label_moon[mask]\n",
    "\n",
    "\n",
    "plt.scatter(X_moons[:, 0][label_moon == 0], X_moons[:, 1][label_moon == 0], color='orangered', marker='.', linewidth=0)\n",
    "plt.scatter(X_moons[:, 0][label_moon == 1], X_moons[:, 1][label_moon == 1], color='green', marker='.', linewidth=0)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.savefig(\"./original_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXqZ5KrtzVqM"
   },
   "source": [
    "### Conditional nFlows models\n",
    "\n",
    "Conditional models in nFlows are handled via the `context` arguments. During the definition of each transform layer, the number of conditional variables has to be specified. \n",
    "\n",
    "Then, during training and generation, the context has to be passed as additional input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5rYwUtonxFc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_layers = 10\n",
    "base_dist = StandardNormal(shape=[n_features])\n",
    "\n",
    "transforms = []\n",
    "for i in range(0, n_layers):\n",
    "    transforms.append(\n",
    "        MaskedAffineAutoregressiveTransform(features=n_features, hidden_features=16, context_features=1)\n",
    "    )  # note the context_features argument\n",
    "    transforms.append(ReversePermutation(features=n_features))\n",
    "\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist).to(device)\n",
    "\n",
    "optimizer = optim.Adam(flow.parameters())\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "max_batches = int(X_moons.shape[0] / batch_size)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    permut = np.random.permutation(X_moons.shape[0])\n",
    "    X_moons_shuffle = X_moons[permut]\n",
    "    label_moon_shuffle = label_moon[permut]\n",
    "\n",
    "    for i_batch in range(max_batches):\n",
    "        x = X_moons_shuffle[i_batch * batch_size : (i_batch + 1) * batch_size]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        y = label_moon_shuffle[i_batch * batch_size : (i_batch + 1) * batch_size]\n",
    "        y = torch.tensor(y, device=device).float().view(-1, 1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        nll = -flow.log_prob(x, context=y)  # note the context argument\n",
    "\n",
    "        loss = nll.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print('Epoch: {:d}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_QgmwT5zGI9"
   },
   "source": [
    "Now lets see how well our model performs, first by using random labels. This should look like the default two moons set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNcs7DP-p4ON",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "cond = np.random.randint(2, size=(n_samples, 1))\n",
    "cond = torch.tensor(cond).float().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples = flow.sample(1, context=cond).view(n_samples, -1).cpu().numpy()\n",
    "\n",
    "print(samples.shape)\n",
    "\n",
    "# plot the results\n",
    "plt.scatter(samples[:, 0], samples[:, 1], color='darkblue', marker='o', linewidth=0)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], color='black', marker='.', linewidth=0)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jO8tZDAYbYj3"
   },
   "source": [
    "We can also test how well our conditioning performs by passing only the label 0 or only label 1 to our network. \n",
    "\n",
    "**Task: Complete the code below to generate points from the two moons separately, and then plot the two moons in separate colors**\n",
    "\n",
    "**Bonus task: What happens when the flow is evaluated using labels different from 0 and 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1mTh2tCroPU"
   },
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "\n",
    "######\n",
    "# Modifications here\n",
    "samples1 = ...\n",
    "samples0 = ...\n",
    "\n",
    "plt.scatter(samples0[:, 0], samples0[:, 1], color='orangered', marker='.', linewidth=0)\n",
    "plt.scatter(samples1[:, 0], samples1[:, 1], color='green', marker='.', linewidth=0)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.savefig(\"./sampled_two_moons.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "159Uova_QyCMPi8ar-y-V2ouzWSpztUK1",
     "timestamp": 1669211085641
    },
    {
     "file_id": "1tfeaFYKTmHLOnICVp_Aw_EFSpiqmQ_Cg",
     "timestamp": 1669133606638
    },
    {
     "file_id": "1iBnBglMcvm4ajE6tde6OnUfxHvqHeWYq",
     "timestamp": 1668762589979
    },
    {
     "file_id": "1OGUkPIhnEgDFUgoiSb8UlbrNi0WiSUSy",
     "timestamp": 1615198767494
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ML4FP2025_Day4",
   "language": "python",
   "name": "training_env_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
